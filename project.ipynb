{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9205e67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f313b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3929 image/mask pairs.\n",
      "\n",
      "Image: /home/tyler/projects/UNet/data/lgg-mri-segmentation/kaggle_3m/TCGA_DU_8164_19970111/TCGA_DU_8164_19970111_1.tif\n",
      "Mask: /home/tyler/projects/UNet/data/lgg-mri-segmentation/kaggle_3m/TCGA_DU_8164_19970111/TCGA_DU_8164_19970111_1_mask.tif\n",
      "Image shape: (256, 256, 3)\n",
      "Mask shape: (256, 256)\n",
      "Tumor images: 1373\n",
      "No-tumor images: 2556\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_dataset(root_dir):\n",
    "    \"\"\"\n",
    "    Recursively parse the LGG MRI dataset.\n",
    "    Returns a list of dictionaries:\n",
    "    [\n",
    "      {\n",
    "        \"image_path\": \"...\",\n",
    "        \"mask_path\": \"...\",\n",
    "        \"image\": np.ndarray,\n",
    "        \"mask\": np.ndarray\n",
    "      },\n",
    "      ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    # Walk through all subdirectories\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        # Only consider tif files\n",
    "        tifs = [f for f in files if f.endswith(\".tif\")]\n",
    "\n",
    "        # Create mapping: index -> file\n",
    "        image_files = {f.replace(\"_mask\", \"\"): f for f in tifs if \"_mask\" not in f}\n",
    "        mask_files  = {f.replace(\"_mask\", \"\"): f for f in tifs if \"_mask\" in f}\n",
    "\n",
    "        # Pair image + mask files\n",
    "        for key in sorted(image_files.keys()):\n",
    "            if key in mask_files:\n",
    "                img_path = os.path.join(subdir, image_files[key])\n",
    "                mask_path = os.path.join(subdir, mask_files[key])\n",
    "\n",
    "                # Load arrays\n",
    "                img = np.array(Image.open(img_path))\n",
    "                mask = np.array(Image.open(mask_path))\n",
    "\n",
    "                data.append({\n",
    "                    \"image_path\": img_path,\n",
    "                    \"mask_path\": mask_path,\n",
    "                    \"image\": img,\n",
    "                    \"mask\": mask\n",
    "                })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = \"/home/tyler/projects/UNet/data/lgg-mri-segmentation/kaggle_3m\"\n",
    "    dataset = load_dataset(root)\n",
    "\n",
    "    print(f\"Loaded {len(dataset)} image/mask pairs.\\n\")\n",
    "\n",
    "    # Example: print one sample\n",
    "    sample = dataset[0]\n",
    "    print(\"Image:\", sample[\"image_path\"])\n",
    "    print(\"Mask:\", sample[\"mask_path\"])\n",
    "    print(\"Image shape:\", sample[\"image\"].shape)\n",
    "    print(\"Mask shape:\", sample[\"mask\"].shape)\n",
    "\n",
    "    # ---- Split into tumor / non-tumor ----\n",
    "    tumor = []\n",
    "    no_tumor = []\n",
    "    \n",
    "    for item in dataset:\n",
    "        if np.any(item[\"mask\"] > 0):\n",
    "            tumor.append(item)\n",
    "        else:\n",
    "            no_tumor.append(item)\n",
    "    \n",
    "    print(\"Tumor images:\", len(tumor))\n",
    "    print(\"No-tumor images:\", len(no_tumor))\n",
    "    \n",
    "    images = np.stack([\n",
    "        np.transpose(item[\"image\"], (2, 0, 1))  # (H,W,C) â†’ (C,H,W)\n",
    "        for item in dataset\n",
    "    ]).astype(np.float32)                        # final shape [N, 3, 256, 256]\n",
    "\n",
    "    masks = np.stack([\n",
    "        item[\"mask\"]                             # (H,W)\n",
    "        for item in dataset\n",
    "    ]).astype(np.float32)                          # final shape [N, 256, 256]\n",
    "    images = np.ascontiguousarray(images)\n",
    "    masks = np.ascontiguousarray(masks)\n",
    "    N, C, H, W = images.shape\n",
    "    shape = [float(N), float(C), float(H), float(W)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86fede38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/tyler/projects/UNet/.venv/lib/python3.12/site-packages/neural_net.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN5Layer19forward_convolutionEPfib",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneural_net\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NeuralNet\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneural_net\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayerType\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mneural_net\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayerDesc\n",
      "\u001b[31mImportError\u001b[39m: /home/tyler/projects/UNet/.venv/lib/python3.12/site-packages/neural_net.cpython-312-x86_64-linux-gnu.so: undefined symbol: _ZN5Layer19forward_convolutionEPfib"
     ]
    }
   ],
   "source": [
    "from neural_net import NeuralNet\n",
    "from neural_net import LayerType\n",
    "from neural_net import LayerDesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6fbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recieved images\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "in_height = 256\n",
    "in_width = 256\n",
    "in_depth = 3\n",
    "\n",
    "UNET = NeuralNet()   # instantiate the C++ class\n",
    "UNET.create([\n",
    "    LayerDesc(LayerType.CONV_LAYER,[in_height, in_width, in_depth, 1, 64, 3,1,1], []), #0\n",
    "    LayerDesc(LayerType.MAX_POOL_LAYER, [2,2],[0]), #1\n",
    "    LayerDesc(LayerType.CONV_LAYER, [in_height//2, in_width//2, in_depth, 64, 128, 3,1,1],[1]),#2\n",
    "    LayerDesc(LayerType.MAX_POOL_LAYER, [2,2],[2]), #3\n",
    "    LayerDesc(LayerType.CONV_LAYER, [in_height//4, in_width//4, in_depth, 128, 256, 3,1,1],[3]), #4\n",
    "    LayerDesc(LayerType.MAX_POOL_LAYER, [2,2],[4]), #5\n",
    "    LayerDesc(LayerType.CONV_LAYER, [in_height//8, in_width//8, in_depth,256, 512, 3,1,1],[5]), #6\n",
    "    #parameters are h_in, w_in, c_in, c_out, stride/upscale\n",
    "    LayerDesc(LayerType.UPSAMPLING_LAYER, [in_height//8, in_width//8, 512, 256, 2],[6]), #7\n",
    "    #parameters are fully determined by parents. First parent is skip connection, second is convolution below \n",
    "    LayerDesc(LayerType.ATTENTION_LAYER, [],[4,6]), #8\n",
    "    #parameters are fully determined by parents. First parent is from skip/attention, second is from convolution below\n",
    "    LayerDesc(LayerType.CONCAT_LAYER, [],[8,7]), #9\n",
    "    LayerDesc(LayerType.CONV_LAYER, [in_height//4, in_width//4, in_depth,512, 128, 3,1,1],[9]), #10\n",
    "    LayerDesc(LayerType.UPSAMPLING_LAYER, [in_height//4, in_width//4, 128, 128, 2],[10]), #11\n",
    "    LayerDesc(LayerType.ATTENTION_LAYER, [],[2,10]), #12 \n",
    "    LayerDesc(LayerType.CONCAT_LAYER, [],[12,11]), #13\n",
    "    LayerDesc(LayerType.CONV_LAYER, [in_height//2, in_width//2, in_depth,256, 64, 3,1,1],[13]), #14\n",
    "    LayerDesc(LayerType.UPSAMPLING_LAYER, [in_height//2, in_width//2, 64, 64, 2],[14]), #15\n",
    "    LayerDesc(LayerType.ATTENTION_LAYER, [],[0,14]), #16 \n",
    "    LayerDesc(LayerType.CONCAT_LAYER, [],[16,15]), #17\n",
    "    LayerDesc(LayerType.CONV_LAYER, [in_height, in_width, in_depth,128, 1, 3,1,1],[17]), #18\n",
    "])\n",
    "\n",
    "\n",
    "learning_rate = .001\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "UNET.train(images,shape,masks,learning_rate, epochs, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a997b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "3\n",
      "245.5625\n"
     ]
    }
   ],
   "source": [
    "print(dataset[2][\"image\"].shape)\n",
    "print(dataset[2][\"image\"][40][30][1])\n",
    "print(len(dataset)/16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dee3ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
